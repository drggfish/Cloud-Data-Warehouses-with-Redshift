## Data Warehouses in Redshift

This repository demonstrates setting up an ETL pipeline on AWS Redshift.  The data used for the pipeline is stored in multiple S3 buckets.  The data are first batch copied from the S3 buckets into staging tables on Redshift, and then further processed into a fact table and multiple dimension tables on the same Redshift cluster. 


## Song Dataset
The first dataset is a subset of real data from the Million Song Dataset.
Each file is in JSON format and contains metadata about a song and the artist of that song.
The files are partitioned by the first three letters of each song's track ID.
For example, here are filepaths to two files in this dataset.
```
song_data/A/B/C/TRAACCG128F92E8A55.json
song_data/A/A/B/TRAACER128F4290F96.json
```

Here is an example of the contents of a single song file, TRAACCG128F92E8A55.json:
```
{
    "num_songs": 1,
    "artist_id": "AR5KOSW1187FB35FF4",
    "artist_latitude": 49.80388,
    "artist_longitude": 15.47491,
    "artist_location": "Dubai UAE",
    "artist_name": "Elena",
    "song_id": "SOZCTXZ12AB0182364",
    "title": "Setanta matins",
    "duration": 269.58322,
    "year": 0
}
```

## Log dataset

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
```
log_data/2018/11/2018-11-01-events.json
log_data/2018/11/2018-11-02-events.json
```

Here is an example of the contents of a single log file, 2018-11-01-events.json:
```
{
	"artist": null,
	"auth": "Logged In",
	"firstName": "Walter",
	"gender": "M",
	"itemInSession": 0,
	"lastName": "Frye",
	"length": null,
	"level": "free",
	"location": "San Francisco-Oakland-Hayward, CA",
	"method": "GET",
	"page": "Home",
	"registration": 1540919166796.0,
	"sessionId": 38,
	"song": null,
	"status": 200,
	"ts": 1541105830796,
	"userAgent": "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"",
	"userId": "39"
}
```

## Staging Tables

The **song_data** and the **log_data** are copied from S3 into two staging tables **staging_events** and **staging_songs**.  These tables are to stage data from the S3 Bucket before being transformed and inserted into the tables that will be used for OLAT processing.

The **staging_events** table contains the following fields with their respective data types:

| Field           | Data Type    |
|-------------    | ------------ |
| artists         | VARCHAR      |
| auth            | VARCHAR      |
| first_name      | VARCHAR      |
| gender          | VARCHAR      |
| item_in_session | INTEGER      |
| last_name       | VARCHAR      |
| length          | FLOAT        |
| level           | VARCHAR      |
| location        | VARCHAR      |
| method          | VARCHAR      |
| page            | VARCHAR      |
| registration    | FLOAT        |
| session_id      | INTEGER      |
| songs           | VARCHAR      |
| status          | INTEGER      |
| ts              | TIMESTAMP    |
| user_agent      | VARCHAR      |
| user_id         | INTEGER      |

The **staging_songs** table contains the following fields with their respective data types:

| Field            | Data Type    |
|-------------     | ------------ |
| artist_id        | VARCHAR      |
| artist_latitude  | NUMERIC      |
| artist_location  | VARCHAR      |
| artist_longitude | NUMERIC      |
| artist_name      | VARCHAR      |
| duration         | FLOAT        |
| num_songs        | INTEGER      |
| song_id          | VARCHAR      |
| title            | VARCHAR      |
| year             | INTEGER      |

## Fact and Dimension Tables

The **staging_events** and **staging_songs** tables are used to load the **Fact** and **Dimension** tables that will be used for OLAT processing. The following tables show the schema of these tables.  The first four tables represent the **Dimension** tables. The last table, **songplay**, represents the **Fact** table.

The **users** table contains the following fields with their respective data types:

| Field            | Data Type    |
|-------------     | ------------ |
| user_id          | INTEGER      |
| first_name       | VARCHAR      |
| last_name        | VARCHAR      |
| gender           | VARCHAR      |
| level            | VARCHAR      |

The **songs** table contains the following fields with their respective data types:

| Field            | Data Type    |
|-------------     | ------------ |
| song_id          | INTEGER      |
| title            | VARCHAR      |
| artist_id        | VARCHAR      |
| artist_name      | VARCHAR      |
| year             | VARCHAR      |
| duration         | FLOAT        |

The **artists** table contains the following fields with their respective data types:

| Field            | Data Type    |
|-------------     | ------------ |
| artist_id        | VARCHAR      |
| artist_name      | VARCHAR      |
| location         | VARCHAR      |
| latitude         | NUMERIC      |
| longitude        | NUMERIC      |

The **time** table contains the following fields with their respective data types:

| Field            | Data Type    |
|-------------     | ------------ |
| start_time       | TIMESTAMP    |
| hour             | INTEGER      |
| day              | INTEGER      |
| week             | INTEGER      |
| month            | INTEGER      |
| year             | INTEGER      |
| weekday          | VARCHAR(10)  |

The **songplays** table contains the following fields with their respective data types:

| Field            | Data Type    |
|-------------     | ------------ |
| songplay_id      | INTEGER      |
| start_time       | TIMESTAMP    |
| user_id          | VARCHAR      |
| level            | VARCHAR      |
| song_id          | VARCHAR      |
| artist_id        | VARCHAR      |
| session_id       | VARCHAR      |
| location         | VARCHAR      |
| user_agent       | VARCHAR      |

The **Dimension** tables and the **Fact** table as described in the **sql_queries.py** file enforce the **primary/foreign key relationships** between the tables as well as the **Redshift** table properties for defining **DISTKEY** and **SORTKEY** for columns.

## File Description

- IAC.ipynb
- ** Infrastructure as Code** The file is used to set up the Redshift cluster and necessary Redshift IAM roles.  The AWS **key** and **secret** are loaded from environmental variables on my machine. This file can also be used to delete the Redshift cluster and IAM roles at the end of activities.

- create_tables.py
- This file is used to create all of the necessary staging and user tables in Redshift.

- etl.py
- This file is used to transfer data from S3 Buckets to staging tables and then to insert data from the staging tables into the Fact and Dimension tables.

- sql_queries.py
- This file contains all of the necessary **SQL** queries used to create all of the Redshift tables and subsequently load them with their required data.

- aws_data_test.ipynb
- This file is used to check the if tables have been successfully created in Redshift and to check if data was properly loaded from S3 buckets to the staging tables and from staging tables to the use tables.

- dwh.cfg
- This is the configuration file which stores all of the necessary settings for launching the AWS Redshift cluster.

- log_json_path.json
- If you use the Udacity S3 data paths this file will be provided. It is used in the **staging_events_copy** routine to copy the source data to the table columns. If you are using this repository and your on S3 buckets, this file will need to be present.

## Order of Operation for building the Repository

1) Set up environment variables required on your host computer.

2) IAC.ipynb until the cell preceded by **Delete cluster when no longer needed**.  The cells following this markdown comment can be ran when you are ready to clean up the AWS environment.

3) create_tables.py

4) etl.py

5) aws_data_test.ipynb

6) To shut down the AWS environment run the lines after the markdown comments **Delete cluster when no longer needed**